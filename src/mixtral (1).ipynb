{"cells":[{"cell_type":"markdown","metadata":{"id":"OW1moHJ1TdhO"},"source":["# Mixtral in Colab\n","\n","Welcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy.\n","\n","To learn more, read our [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub."]},{"cell_type":"markdown","metadata":{"id":"2-dvAX_hKZT4"},"source":["One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n","\n","\n","<details>\n","\n","<summary>How to balance between RAM and GPU VRAM usage</summary>\n","\n","You can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n","\n","Note that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"Y8MhvkC7TKEL"},"source":["## Install and import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7qY7ebqX7T7"},"outputs":[],"source":["# fix numpy in colab\n","import numpy\n","from IPython.display import clear_output\n","\n","# fix triton in colab\n","!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia\n","\n","!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n","!cd mixtral-offloading && pip install -q -r requirements.txt\n","\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6838,"status":"ok","timestamp":1713127936339,"user":{"displayName":"Maria BOSLI","userId":"07921029930901901317"},"user_tz":-120},"id":"GgpjnV7fV49W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c247129a-7773-4d4e-c126-1737860c848b"},"outputs":[{"output_type":"stream","name":"stdout","text":["hqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n"]}],"source":["import sys\n","\n","sys.path.append(\"mixtral-offloading\")\n","import torch\n","from torch.nn import functional as F\n","from hqq.core.quantize import BaseQuantizeConfig\n","from huggingface_hub import snapshot_download\n","from IPython.display import clear_output\n","from tqdm.auto import trange\n","from transformers import AutoConfig, AutoTokenizer\n","from transformers.utils import logging as hf_logging\n","\n","from src.build_model import OffloadConfig, QuantConfig, build_model\n","\n","hf_logging.disable_progress_bar()"]},{"cell_type":"markdown","metadata":{"id":"OkSYibHcTQsH"},"source":["## Initialize model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["c090fa405bcd42fca4b73418eb1daae9","a8125e9010044714bb372d83b10a0e39","252a7b0f4479415c9ec32a51934a5328","ce0448e92ea64b7e90396ac0231c7c4a","757be0f6905344c08e007a2b597c8bb6","0651fe9804154f6587e60dbbe5bc1ffd","3885b89a69d04dd5bb142203b023bb4d","0562c95fd0a24b1d9f07027dad018f17","de139a9d14db46f98429b348bb816ecb","872cf83de9bd4d439e9006034485b196","fa8566cd428a46e0951145623329cb5c"]},"executionInfo":{"elapsed":105497,"status":"ok","timestamp":1713128054896,"user":{"displayName":"Maria BOSLI","userId":"07921029930901901317"},"user_tz":-120},"id":"_mIpePTMFyRY","outputId":"c63a326e-24a3-4d7b-cf28-1ab58cd1319b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"display_data","data":{"text/plain":["Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c090fa405bcd42fca4b73418eb1daae9"}},"metadata":{}}],"source":["model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","\n","\n","\n","config = AutoConfig.from_pretrained(quantized_model_name)\n","state_path = snapshot_download(quantized_model_name)\n","\n","device = torch.device(\"cuda:0\")\n","\n","##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n","offload_per_layer = 4\n","# offload_per_layer = 5\n","###############################################################\n","\n","num_experts = config.num_local_experts\n","\n","offload_config = OffloadConfig(\n","    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n","    offload_size=config.num_hidden_layers * offload_per_layer,\n","    buffer_size=4,\n","    offload_per_layer=offload_per_layer,\n",")\n","\n","\n","attn_config = BaseQuantizeConfig(\n","    nbits=4,\n","    group_size=64,\n","    quant_zero=True,\n","    quant_scale=True,\n",")\n","attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n","\n","\n","ffn_config = BaseQuantizeConfig(\n","    nbits=2,\n","    group_size=16,\n","    quant_zero=True,\n","    quant_scale=True,\n",")\n","quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n","\n","\n","model = build_model(\n","    device=device,\n","    quant_config=quant_config,\n","    offload_config=offload_config,\n","    state_path=state_path,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Z4hBFYtPTUzT"},"source":["## Run the model"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/My Drive/MixMatch/keywords.txt'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MiMWZ9nfUkE","executionInfo":{"status":"ok","timestamp":1713128096820,"user_tz":-120,"elapsed":24472,"user":{"displayName":"Maria BOSLI","userId":"07921029930901901317"}},"outputId":"b9eba3c9-423b-4a5b-a85c-0b2c916e3ba7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import re\n","from transformers import TextStreamer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","past_key_values = None\n","sequence = None\n","\n","seq_len = 0\n","\n","# Lire le contenu de fichier bigrams.txt par exemple\n","with open(\"/content/drive/MyDrive/topic/bigrams.txt\", \"r\", encoding=\"utf-8\") as f:\n","    bigrams_content = f.read()\n","\n","\n","# Construire le prompt avec le contenu de fichier\n","prompt = f\"Generate topics based on the list of the most frequent bigrams. For each topic, provide a summary description of what was discussed:\\n{bigrams_content}\\n\\n\"\n","user_entry = dict(role=\"user\", content=prompt)\n","input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n","\n","if past_key_values is None:\n","    attention_mask = torch.ones_like(input_ids)\n","else:\n","    seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n","    attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n","\n","print(\"Mixtral: \", end=\"\")\n","result = model.generate(\n","    input_ids=input_ids,\n","    attention_mask=attention_mask,\n","    past_key_values=past_key_values,\n","    streamer=streamer,\n","    do_sample=True,\n","    temperature=0.9,\n","    top_p=0.9,\n","    max_new_tokens=512,\n","    pad_token_id=tokenizer.eos_token_id,\n","    return_dict_in_generate=True,\n","    output_hidden_states=True,\n",")\n","\n","# Enregistrer le rÃ©sultat de Mixtral dans une variable temporaire\n","generated_text = tokenizer.decode(result[\"sequences\"][0], skip_special_tokens=True)\n","\n","# Supprimer tout le texte entre [INST] et [/INST] ainsi que ces balises elles-mÃªmes\n","prompt = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', prompt).strip()\n","generated_text = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', generated_text).strip()\n","\n","# Ajouter le code pour enregistrer les prompts et les rÃ©sultats\n","with open(file_path, 'a', encoding='utf-8') as f:\n","    f.write(\"Prompt: {}\\n\".format(prompt))\n","    f.write(\"Mixtral: {}\\n\".format(generated_text))\n","    f.write(\"\\n\")\n","print(\"\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUaXduqtTnak","executionInfo":{"status":"ok","timestamp":1712858602858,"user_tz":-120,"elapsed":313264,"user":{"displayName":"Maria BOSLI","userId":"07921029930901901317"}},"outputId":"ba3baaa9-21b3-490b-c4a0-eb0eccdadaa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mixtral: Topic: COVID-19 and Chest Imaging\n","Summary: A significant amount of discussion revolves around the use of chest imaging, particularly CT scans, in diagnosing and managing COVID-19. The sensitivity and specificity of chest imaging are compared to RT-PCR tests, with some experts suggesting that CT scans may have a higher sensitivity in detecting COVID-19, particularly in patients with mild symptoms or in the early stages of the disease. However, the specificity of chest imaging may be lower than that of RT-PCR tests, and the risk of radiation exposure is a concern. The use of chest imaging in clinical settings is also discussed, including its role in monitoring disease progression and evaluating the severity of COVID-19.\n","\n","Topic: RT-PCR Tests and COVID-19 Diagnosis\n","Summary: Another prominent topic is the use of RT-PCR tests in diagnosing COVID-19. The accuracy, sensitivity, and specificity of RT-PCR tests are discussed, as well as the challenges and limitations of these tests, such as the possibility of false positive or false negative results. The timing of the test and the clinical presentation of the patient are also considered important factors that can affect the accuracy of RT-PCR tests.\n","\n","Topic: COVID-19 Clinical Symptoms and Manifestations\n","Summary: The discussion also covers the clinical symptoms and manifestations of COVID-19, including viral pneumonia, ground-glass opacity, and halo sign. The typical and atypical CT findings of COVID-19 are compared, and the utility of CT scans in diagnosing COVID-19 is debated. The correlation between the clinical presentation and the radiological findings of COVID-19 is also explored.\n","\n","Topic: COVID-19 Research and Publication\n","Summary: Lastly, several entries refer to recent research and publications related to COVID-19, such as studies on the diagnostic accuracy of chest imaging, the clinical features of COVID-19, and the role of RT-PCR tests in confirming the diagnosis. The publication venues and the authors of these studies are also mentioned.\n","\n","\n"]}]},{"cell_type":"code","source":["import re\n","from transformers import TextStreamer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","past_key_values = None\n","sequence = None\n","\n","seq_len = 0\n","\n","while True:\n","    print(\"User: \", end=\"\")\n","    user_input = input()\n","    print(\"\\n\")\n","\n","    user_entry = dict(role=\"user\", content=user_input)\n","    input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n","\n","    if past_key_values is None:\n","        attention_mask = torch.ones_like(input_ids)\n","    else:\n","        seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n","        attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n","\n","    print(\"Mixtral: \", end=\"\")\n","    result = model.generate(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        past_key_values=past_key_values,\n","        streamer=streamer,\n","        do_sample=True,\n","        temperature=0.9,\n","        top_p=0.9,\n","        max_new_tokens=512,\n","        pad_token_id=tokenizer.eos_token_id,\n","        return_dict_in_generate=True,\n","        output_hidden_states=True,\n","    )\n","\n","    # Enregistrer le rÃ©sultat de Mixtral dans une variable temporaire\n","    input_text = user_input\n","    generated_text = tokenizer.decode(result[\"sequences\"][0], skip_special_tokens=True)\n","\n","    # Supprimer tout le texte entre [INST] et [/INST] ainsi que ces balises elles-mÃªmes\n","    generated_text = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', generated_text).strip()\n","\n","    # Ajouter le code pour enregistrer les prompts et les rÃ©sultats\n","    with open(file_path, 'a', encoding='utf-8') as f:\n","        f.write(\"Prompt: {}\\n\".format(user_input))\n","        f.write(\"Mixtral: {}\\n\".format(generated_text))\n","        f.write(\"\\n\")\n","    print(\"\\n\")\n","\n","    sequence = result[\"sequences\"]\n","    past_key_values = result[\"past_key_values\"]\n"],"metadata":{"id":"UeQy-_6Y3N_W"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c090fa405bcd42fca4b73418eb1daae9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8125e9010044714bb372d83b10a0e39","IPY_MODEL_252a7b0f4479415c9ec32a51934a5328","IPY_MODEL_ce0448e92ea64b7e90396ac0231c7c4a"],"layout":"IPY_MODEL_757be0f6905344c08e007a2b597c8bb6"}},"a8125e9010044714bb372d83b10a0e39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0651fe9804154f6587e60dbbe5bc1ffd","placeholder":"â","style":"IPY_MODEL_3885b89a69d04dd5bb142203b023bb4d","value":"Loading experts: 100%"}},"252a7b0f4479415c9ec32a51934a5328":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0562c95fd0a24b1d9f07027dad018f17","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de139a9d14db46f98429b348bb816ecb","value":32}},"ce0448e92ea64b7e90396ac0231c7c4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_872cf83de9bd4d439e9006034485b196","placeholder":"â","style":"IPY_MODEL_fa8566cd428a46e0951145623329cb5c","value":" 32/32 [00:10&lt;00:00,  2.89it/s]"}},"757be0f6905344c08e007a2b597c8bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0651fe9804154f6587e60dbbe5bc1ffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3885b89a69d04dd5bb142203b023bb4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0562c95fd0a24b1d9f07027dad018f17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de139a9d14db46f98429b348bb816ecb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"872cf83de9bd4d439e9006034485b196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa8566cd428a46e0951145623329cb5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}